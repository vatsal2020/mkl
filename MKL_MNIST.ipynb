{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import argparse\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the neural network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 15, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(15, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        #x = F.relu(F.max_pool2d(self.conv2(x), 2, stride=2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining noisy training loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "A simple way to define a noisy data loader. We pick a batch of training\n",
    "set size and introduce corruptions via directed or random noise models \n",
    "on randomly chosen samples\n",
    "'''\n",
    "def noisy_loader(params):\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "                        datasets.MNIST('../data', train=True, download=True,\n",
    "                                       transform=transforms.Compose([transforms.ToTensor(),\n",
    "                                                                     transforms.Normalize((0.1307,), (0.3081,))])),\n",
    "                                       batch_size=params['batchsize'], shuffle=True)\n",
    "    for index, (data, target) in enumerate(train_loader):\n",
    "        flag = np.random.binomial(1, params['epsilon'], size=(len(target), 1))\n",
    "        target_noisy = copy.deepcopy(target.numpy())\n",
    "        if params['clean_data'] is False:\n",
    "            for index, val in enumerate(flag):\n",
    "                if val[0] == 1 and params['noise_type'] == 'directed':\n",
    "                    target_noisy[index] = (target[index] + params['shift']) % 10\n",
    "                if val[0] == 1 and params['noise_type'] == 'random':\n",
    "                    out = np.random.randint(0, 10)\n",
    "                    while out == target_noisy[index]:\n",
    "                        out = np.random.randint(0, 10)\n",
    "                    target_noisy[index] = out\n",
    "            break\n",
    "        elif params['clean_data'] is True:\n",
    "            print('Do nothing')\n",
    "            \n",
    "    target_noisy = torch.from_numpy(target_noisy)\n",
    "    train_noisy = torch.utils.data.TensorDataset(data, target_noisy)\n",
    "\n",
    "    if params['optimizer_type'] == 'sgd':\n",
    "        train_loader_noisy = torch.utils.data.DataLoader(train_noisy, batch_size=int(params['frac'] * params['k']), \n",
    "                                                         shuffle=True, drop_last=True)\n",
    "    elif params['optimizer_type'] == 'mkl':\n",
    "        train_loader_noisy = torch.utils.data.DataLoader(train_noisy, batch_size=params['k'], shuffle=True, \n",
    "                                                         drop_last=True)\n",
    "    return train_loader_noisy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Testing Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader_noisy, epoch, run, epsilon, params):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader_noisy):\n",
    "        data, target = Variable(data.cuda(), requires_grad=True), Variable(target.cuda())\n",
    "        if params['optimizer_type'] == 'sgd':\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "        elif params['optimizer_type'] == 'mkl':\n",
    "            output = model(data)\n",
    "            temp_loss = F.nll_loss(output, target.cuda(), reduction='none')\n",
    "            temp = temp_loss.cpu().detach().numpy() \n",
    "            # Pick the samples with the lowest loss\n",
    "            index1 = np.argpartition(temp, int(params['frac'] * params['k']))\n",
    "            data1 = data[index1[:int(params['frac'] * params['k'])],:,:,:].view(int(params['frac'] * params['k']), 1, 28, 28)\n",
    "            target1 = target[index1[:int(params['frac'] * params['k'])]]\n",
    "            data1, target1 = Variable(data1.cuda()), Variable(target1.cuda())\n",
    "            output1 = model(data1)\n",
    "            optimizer.zero_grad()\n",
    "            loss = F.nll_loss(output1, target1)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 250 == 249:\n",
    "            print('Train Run: {} Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}, {}'.format(\n",
    "                run + 1 , epoch, batch_idx * len(data), len(train_loader_noisy.dataset),\n",
    "                100. * batch_idx / len(train_loader_noisy), loss.item(), params['optimizer_type']))\n",
    "    return loss.item()\n",
    "\n",
    "def test(test_loader, run):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, size_average=False).item() # sum up batch loss\n",
    "            pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.data.view_as(pred)).float().cpu().sum()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set {} Run: {} : Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "            params['optimizer_type'], run + 1, test_loss, correct, len(test_loader.dataset),\n",
    "            100. * correct / len(test_loader.dataset)))\n",
    "    return test_loss, 100. * correct / len(test_loader.dataset)\n",
    "\n",
    "def select_optimizer(optimizer_name, params):\n",
    "    if optimizer_name == 'sgd':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=params['lr'])\n",
    "    elif optimizer_name == 'sgdmomentum':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=params['lr'], momentum=params['momentum'])\n",
    "    return optimizer\n",
    "\n",
    "def one_run(train_loader_noisy, model, optimizer, params, results):\n",
    "    num_epochs = params['num_epochs']\n",
    "    count = params['current_run']\n",
    "    time_start = time.time()\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        if params['decayschedule'] != 0:\n",
    "            scheduler.step()\n",
    "        results['train_loss'][epoch - 1, count] = train(train_loader_noisy, epoch, params['current_run'], params['epsilon'], params)\n",
    "        results['test_loss'][epoch - 1, count], results['test_acc'][epoch - 1, count] = test(test_loader, params['current_run'])\n",
    "        results['time_spent'][epoch - 1, count] = time.time() - time_start\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params():\n",
    "    params = {}\n",
    "    params['lr'] = 0.05                # Learning rate\n",
    "    params['momentum'] = 0.0           # Momentum parameter\n",
    "    params['k'] = 10                   # Number of loss evaluations per batch\n",
    "    params['batchsize'] = 5000         # Size of dataset, parameter used in noisy_loader\n",
    "    params['decayschedule'] = 30       # Decay Schedule\n",
    "    params['noise_type'] = 'directed'  # Noise type\n",
    "                                       #    To use directed noise model of corruption, set 'directed'\n",
    "                                       #    To use random noise model of corruption, set 'random'\n",
    "    params['learningratedecay'] = 0.2  # Learning Rate Decay\n",
    "    params['eps']= 1e-08               # Corruption parameter\n",
    "    params['num_epochs'] = 80          # Number of epochs\n",
    "    params['optimizer_type'] = 'mkl'   # Optimizer Type:\n",
    "                                       #    To run standard stochastic gradient descent, set 'sgd'\n",
    "                                       #    To run standard MKL-SGD, set 'mkl'\n",
    "    params['runs'] = 5                 # Number of runs\n",
    "    params['epsilon'] = 0.1            # Fraction of corrupted data \n",
    "    params['frac'] = 0.6               # Fraction of samples with lowest loss chosen\n",
    "                                       #    Number of gradient updates = params['frac'] * params['k']\n",
    "    params['clean_data'] = False       # Flag to only consider clean data\n",
    "    params['shift'] = 2                # Shift parameter in directed noise model\n",
    "\n",
    "    results = {}\n",
    "    results['train_loss'] = np.zeros((params['num_epochs'], params['runs']))\n",
    "    results['test_loss'] = np.zeros((params['num_epochs'], params['runs']))\n",
    "    results['test_acc'] = np.zeros((params['num_epochs'], params['runs']))\n",
    "    results['time_spent'] = np.zeros((params['num_epochs'], params['runs'])) \n",
    "\n",
    "    transform_train = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "    transform_test = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "    trainset = datasets.MNIST('../data', train=True, download=True, transform=transform_train)\n",
    "    testset = datasets.MNIST('../data', train=False, transform = transform_test)\n",
    "    test_loader = torch.utils.data.DataLoader(testset,batch_size = 1000, shuffle=False)\n",
    "    return params, trainset, test_loader, results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "params, trainset, test_loader, results = init_params()\n",
    "for run in range(params['runs']):\n",
    "    train_loader_noisy = noisy_loader(params)\n",
    "    model = Net()\n",
    "    model.cuda()\n",
    "    optimizer = select_optimizer('sgd', params)\n",
    "    if params['decayschedule'] != 0:\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=params['decayschedule'], gamma=params['learningratedecay'])\n",
    "    model = torch.nn.DataParallel(model, device_ids=range(torch.cuda.device_count()))        \n",
    "    params['current_run'] = run\n",
    "    results = one_run(train_loader_noisy, model, optimizer, params, results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if params['optimizer_type'] == 'sgd':\n",
    "    file_path = \"./results/MNIST_SGD_latest_%s_%s/lr_%.4f_momentum_%.4f_eps_%.4f_ds_%d_n_epochs_%d_runs_%d_minibatch_%d\"\\\n",
    "                % (params['optimizer_type'], params['noise_type'], params['lr'], \n",
    "                   params['momentum'], params['epsilon'], params['decayschedule'], \n",
    "                   params['num_epochs'], params['runs'], params['k'])\n",
    "elif params['optimizer_type'] == 'mkl':\n",
    "    file_path = \"./results/MNIST_SGD_latest_%s_%s/lr_%.4f_momentum_%.4f_eps_%.4f_ds_%d_n_epochs_%d_runs_%d_minibatch_%d_frac_%.2f\"\\\n",
    "                % (params['optimizer_type'], params['noise_type'], params['lr'], \n",
    "                   params['momentum'], params['epsilon'], params['decayschedule'], \n",
    "                   params['num_epochs'], params['runs'], params['k'], params['frac'])\n",
    "\n",
    "directory = os.path.dirname(file_path)\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "np.savez(file_path, train_loss=results['train_loss'], test_loss=results['test_loss'], test_acc=results['test_acc'], time_spent=results['time_spent'])\n",
    "\n",
    "  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
